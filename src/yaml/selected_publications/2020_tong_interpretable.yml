abstract: While neural networks are powerful approximators used to classify or embed
  data into lower dimensional spaces, they are often regarded as black boxes with
  uninterpretable features. Here we propose Graph Spectral Regularization for making
  hidden layers more interpretable without significantly impacting performance on
  the primary task. Taking inspiration from spatial organization and localization
  of neuron activations in biological networks, we use a graph Laplacian penalty to
  structure the activations within a layer. This penalty encourages activations to
  be smooth either on a predetermined graph or on a feature-space graph learned from
  the data via co-activations of a hidden layer of the neural network. We show numerous
  uses for this additional structure including cluster indication and visualization
  in biological and image data sets.
authors:
- Alexander Y. Tong
- David van Dijk
- Jay S. Stanley
- Matthew Amodio
- Kristina Yim
- Rebecca Muhle
- James Noonan
- Guy Wolf
- Smita Krishnaswamy
href: https://link.springer.com/chapter/10.1007/978-3-030-44584-3_40
keywords:
- Feature saliency
- Graph learning
- Neural Network Interpretability
pages: 509-521
periodical: Lecture Notes in Computer Science (including subseries Lecture Notes in
  Artificial Intelligence and Lecture Notes in Bioinformatics)
publisher: Springer
selected: true
title: Interpretable Neuron Structuring with Graph Spectral Regularization
type: Journal Article
year: 2020

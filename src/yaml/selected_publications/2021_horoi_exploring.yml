abstract: Recent work has established clear links between the generalization performance
  of trained neural networks and the geometry of their loss landscape near the local
  minima to which they converge. This suggests that qualitative and quantitative examination
  of the loss landscape geometry could yield insights about neural network generalization
  performance during training. To this end, researchers have proposed visualizing
  the loss landscape through the use of simple dimensionality reduction techniques.
  However, such visualization methods have been limited by their linear nature and
  only capture features in one or two dimensions, thus restricting sampling of the
  loss landscape to lines or planes. Here, we expand and improve upon these in three
  ways. First, we present a novel "jump and retrain" procedure for sampling relevant
  portions of the loss landscape. We show that the resulting sampled data holds more
  meaningful information about the network's ability to generalize. Next, we show
  that non-linear dimensionality reduction of the jump and retrain trajectories via
  PHATE, a trajectory and manifold-preserving method, allows us to visualize differences
  between networks that are generalizing well vs poorly. Finally, we combine PHATE
  trajectories with a computational homology characterization to quantify trajectory
  differences.
authors:
- Stefan Horoi
- Jessie Huang
- Bastian Rieck
- Guillaume Lajoie
- Guy Wolf
- Smita Krishnaswamy
href: https://arxiv.org/abs/2102.00485v2
keywords:
- '3'
- '5'
- Artificial neural network loss landscape
- Guillaume Lajoie 1
- Jessie Huang
- Non-linear dimensionality reduction
- Topological data analysis
month: 1
pages: 171-184
periodical: Lecture Notes in Computer Science (including subseries Lecture Notes in
  Artificial Intelligence and Lecture Notes in Bioinformatics)
publisher: Springer Science and Business Media Deutschland GmbH
selected: true
title: Exploring the Geometry and Topology of Neural Network Loss Landscapes
type: Journal Article
year: 2021
